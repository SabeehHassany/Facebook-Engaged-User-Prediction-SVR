{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "support_vector_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9td_QA4BbOV",
        "colab_type": "text"
      },
      "source": [
        "Support Vector Regression is a more advanced version of linear regression. It uses support vectors which are data points not on the line as a way to stabilize itself on the line as well be able to expect vectors as predicted values. This dataset is retrieved from the UCI Machine Learning repository and contains 2014 Facebook post metrics from over 500 posts from an international cosmetics company. It has multiple independent and dependent variables that we can use. In this case, I used the metrics to predict Lifetime Engaged Users as a baseline (although any other predicted variable can be used)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOyqYHTk_Q57",
        "colab_type": "text"
      },
      "source": [
        "### Importing the libraries\n",
        "These are the three go to libraries for most ML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_YHJjnD_Tja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HPrHEiT36pQY"
      },
      "source": [
        "### Importing the dataset\n",
        "I imported the dataset through Pandas dataframe and used iloc to assign the variables. Remember that the name of the dataset has to be updated for diff usecases AND it must be in the same folder as your .py file or uploaded on Jupyter Notebooks or Google Collab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I6kxG70_6pQZ",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv('dataset_Facebook.csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "y = y.reshape(-1, 1)\n",
        "#print(dataset.info())\n",
        "# ^ this is just to see the missing entries in our data. As an alternative you can also use conditional formatting on the csv."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yudNsNxw6pQc"
      },
      "source": [
        "### Missing Data\n",
        "For this dataset there were a total of 3 columns with missing data. For index 8 and 9 (last index is not included hence '8:10') I used the 'median' to impute to compensate for outliers. For index 6 I used 'most_frequent' because it was a binary datapoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O4SQREoi6pQc",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "imputer.fit(X[:, 8:10])\n",
        "X[:, 8:10] = imputer.transform(X[:, 8:10])\n",
        "\n",
        "imputer2 = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
        "imputer2.fit(X[:, 6:7])\n",
        "X[:, 6:7] = imputer2.transform(X[:, 6:7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uNEvN-DN6pQg"
      },
      "source": [
        "### Encoding categorical data\n",
        "Index 1 had categorical data that had to be converted using OneHotEncoding.This must be done AFTER imputing missing values since OneHotEncoding automatically makes the encoded column the first index which displaces all the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "si6wh6Si6pQh",
        "colab": {}
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
        "X = ct.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS8FeLHYS-nI",
        "colab_type": "text"
      },
      "source": [
        "### Feature Scaling\n",
        "For SVR we always have to feature scale our data. Both X and Y. This is because it can help our model find vectors much easier which can help increase efficicy, especially as variables increase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGeAlD1HTDI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "sc_y = StandardScaler()\n",
        "X[:, 4:] = sc_X.fit_transform(X[:, 4:])\n",
        "y = sc_y.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WemVnqgeA70k",
        "colab_type": "text"
      },
      "source": [
        "### Splitting the dataset into the Training set and Test set\n",
        "Because of the large dataset and many variables for a simple algorothim I used a 90/10 split. The random state is tuned to 5 for consistency sakes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb_v_ae-A-20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, random_state = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiU6D2QFRjxY",
        "colab_type": "text"
      },
      "source": [
        "### Training the SVR model on the whole dataset\n",
        "There are a lot of different kernels that we can use with SVR but using a radial basis function (rbf) is best as its optimized for support vector machines and uses euclidean distance measurements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6R4rt_GRz15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "regressor = SVR(kernel = 'rbf')\n",
        "regressor.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TzdEt-9eCU1r"
      },
      "source": [
        "### Predicting the Test set results\n",
        "By using the concatenate function I display the predicted values and  actual values in a side by side 2D array through '(len(y_wtv), 1))' for easy viewing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fmZcxSc3CU1v",
        "colab": {}
      },
      "source": [
        "y_pred = sc_y.inverse_transform(regressor.predict(X_test))\n",
        "y_test = sc_y.inverse_transform(y_test)\n",
        "np.set_printoptions(precision=0, suppress=True)\n",
        "#print(np.concatenate((y_pred.astype(int).reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n",
        "\n",
        "#y_pred = regressor.predict(X_test)\n",
        "#np.set_printoptions(precision=0, suppress=True)\n",
        "#print(np.concatenate((y_pred.astype(int).reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SQTMVXo2MhkX"
      },
      "source": [
        "### Evaluating Model Performance\n",
        "We use two metrics to evaluate our model performance, r^2 being the more superior. These are both simple to understand and are covered in one of my Medium articles! In this model we achieved a .80 r2 which means 80% of our data can be predicted by our linear regression! Even when we use 'random_state = None' which creates a different test_train_split each time, we get upwards of a .70 r2!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiOu2RtBztA7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1ab8cb4a-e4a0-436a-cb40-ec3a19d10318"
      },
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error as mse\n",
        "print(\"r^2: \" + str(r2_score(y_test, y_pred)))\n",
        "print(\"MSE: \" + str(mse(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r^2: 0.8060542709053213\n",
            "MSE: 93540.05376875312\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}